getwd()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)   # plot
library(glmnet)    # lasso
library(grf)       # generalized random forests
library(sandwich)  # for robust CIs
library(devtools)  # install from GitHub
#install_github("swager/balanceHD") #Run this if you have not installed balanceHD before
library(balanceHD) # approximate residual balancing
install.packages("ggplot2")
install.packages("glmnet")
install.packages("grf")
install.packages("sandwich")
install.packages("devtools")
install.packages("devtools")
install.packages("balanceHD")
source('load_data.R')
df_experiment <- select_dataset("welfare")
install.packages("tidyverse", "tidyselect", "dplyr", "fBasics", "corrplot", "psych", "grf", "rpart", "rpart.plot", "treeClust", "car","devtools","readr","tidyr","tibble","knitr","kableExtra","ggplot2","haven","aod","evtree","purrr")
install.packages("tidyverse", "tidyselect", "dplyr", "fBasics", "corrplot", "psych", "grf", "rpart", "rpart.plot", "treeClust", "car","devtools","readr","tidyr","tibble","knitr","kableExtra","ggplot2","haven","aod","evtree","purrr")
install.packages("tidyverse")
install.packages("tidyselect")
install.packages("dplyr")
install.packages("fBasics")
install.packages("corrplot")
install.packages("psych")
install.packages("grf")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("treeClust")
install.packages("car")
install.packages("devtools")
install.packages("readr")
install.packages("tidyr")
install.packages("tibble")
install.packages("knitr")
install.packages("kableExtra")
install.packages("ggplot2")
install.packages("haven")
install.packages("aos")
install.packages("aod")
install.packages("evtree")
install.packages("purrr")
install_github('susanathey/causalTree')
library(devtools)
install_github("susanathey/causalTree")
orce = TRUE
install_github("susanathey/causalTree",force = TRUE)
library(tidyverse)
library(tidyselect)
library(dplyr)       # Data manipulation (0.8.0.1)
library(fBasics)     # Summary statistics (3042.89)
library(corrplot)    # Correlations (0.84)
library(psych)       # Correlation p-values (1.8.12)
library(grf)         # Generalized random forests (0.10.2)
library(rpart)       # Classification and regression trees, or CART (4.1-13)
library(rpart.plot)  # Plotting trees (3.0.6)
library(treeClust)   # Predicting leaf position for causal trees (1.1-7)
library(car)         # linear hypothesis testing for causal tree (3.0-2)
library(devtools)    # Install packages from github (2.0.1)
library(readr)       # Reading csv files (1.3.1)
library(tidyr)       # Database operations (0.8.3)
library(tibble)      # Modern alternative to data frames (2.1.1)
library(knitr)       # RMarkdown (1.21)
library(kableExtra)  # Prettier RMarkdown (1.0.1)
library(ggplot2)     # general plotting tool (3.1.0)
library(haven)       # read stata files (2.0.0)
library(aod)         # hypothesis testing (1.3.1)
library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)
library(purrr)
library(causalTree)
source('load_data.R')
df_experiment <- select_dataset("welfare")
df_experiment <- select_dataset("welfare")
q
df_experiment <- select_dataset("welfare")
df_experiment
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df_experiment %>% select(all_variables_names)
df
df <- df %>% drop_na()
df
df <- df %>% rename(Y=outcome_variable_name,W=treatment_variable_name)
df
df <- data.frame(lapply(df, function(x) as.numeric(as.character(x))))
df <- df %>% mutate_if(is.character,as.numeric)
df <- df %>% rowid_to_column( "ID")
df
train_fraction <- 0.80  # Use train_fraction % of the dataset to train our models
df_train <- sample_frac(df, replace=F, size=train_fraction)
df_test <- anti_join(df,df_train, by = "ID")#need to check on larger datasets
summ_stats <- fBasics::basicStats(df)
summ_stats <- as.data.frame(t(summ_stats))
summ_stats
summ_stats <- summ_stats %>% select("Mean", "Stdev", "Minimum", "1. Quartile", "Median",  "3. Quartile", "Maximum")
summ_stats <- summ_stats %>% rename('Lower quartile'= '1. Quartile', 'Upper quartile' ='3. Quartile')
split_size <- floor(nrow(df_train) * 0.5)
df_split <- sample_n(df_train, replace=FALSE, size=split_size)
df_est <- anti_join(df_train,df_split, by ="ID")
fmla_ct <- paste("factor(Y) ~", paste(covariate_names, collapse = " + "))
print('This is our regression model')
print( fmla_ct)
ct_unpruned <- honest.causalTree(
formula = fmla_ct,            # Define the model
data = df_split,              # Subset used to create tree structure
est_data = df_est,            # Which data set to use to estimate effects
treatment = df_split$W,       # Splitting sample treatment variable
est_treatment = df_est$W,     # Estimation sample treatment variable
split.Rule = "CT",            # Define the splitting option
cv.option = "TOT",            # Cross validation options
cp = 0,                       # Complexity parameter
split.Honest = TRUE,          # Use honesty when splitting
cv.Honest = TRUE,             # Use honesty when performing cross-validation
minsize = 10,                 # Min. number of treatment and control cases in each leaf
HonestSampleSize = nrow(df_est))
# Table of cross-validated values by tuning parameter.
ct_cptable <- as.data.frame(ct_unpruned$cptable)
# Obtain optimal complexity parameter to prune tree.
selected_cp <- which.min(ct_cptable$xerror)
optim_cp_ct <- ct_cptable[selected_cp, "CP"]
# Prune the tree at optimal complexity parameter.
ct_pruned <- prune(tree = ct_unpruned, cp = optim_cp_ct)
tauhat_ct_est <- predict(ct_pruned, newdata = df_est)
tauhat_ct_est
# Create a factor column 'leaf' indicating leaf assignment
num_leaves <- length(unique(tauhat_ct_est))  #There are as many leaves as there are predictions
df_est$leaf <- factor(tauhat_ct_est, labels = seq(num_leaves))
# Run the regression
ols_ct <- lm(as.formula("Y ~ 0 + leaf + W:leaf"), data= df_est) #Warning: the tree won't split for charitable dataset
print(as.formula("Y ~ 0 + leaf + W:leaf"))
ols_ct_summary <- summary(ols_ct)
te_summary <- coef(ols_ct_summary)[(num_leaves+1):(2*num_leaves), c("Estimate", "Std. Error")]
te_summary
tauhat_ct_test <- predict(ct_pruned, newdata=df_test)
tauhat_ct_test
rpart.plot(
x = ct_pruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
X <- df_train[,covariate_names]
W <- df_train$W
Y <- df_train$Y
num.trees <- 200  #  We'll make this a small number for speed here.
n_train <- dim(df_train)[1]
# estimate separate response functions
tf0 <- regression_forest(X[W==0,], Y[W==0], num.trees=num.trees)
tf1 <- regression_forest(X[W==1,], Y[W==1], num.trees=num.trees)
# Compute the 'imputed treatment effects' using the other group
D1 <- Y[W==1] - predict(tf0, X[W==1,])$predictions
D0 <- predict(tf1, X[W==0,])$predictions - Y[W==0]
# Compute the cross estimators
xf0 <- regression_forest(X[W==0,], D0, num.trees=num.trees)
xf1 <- regression_forest(X[W==1,], D1, num.trees=num.trees)
# Predict treatment effects, making sure to always use OOB predictions where appropriate
xf.preds.0 <- rep(0, n_train)
xf.preds.0[W==0] <- predict(xf0)$predictions
xf.preds.0[W==1] <- predict(xf0, X[W==1,])$predictions
xf.preds.1 <- rep(0, n_train)
xf.preds.1[W==0] <- predict(xf0)$predictions
xf.preds.1[W==1] <- predict(xf0, X[W==1,])$predictions
# Estimate the propensity score
propf <- regression_forest(X, W, num.trees=num.trees)
ehat <- predict(propf)$predictions
# Finally, compute the X-learner prediction
tauhat_xl <- (1 - ehat) * xf.preds.1 + ehat * xf.preds.0
X.test <- df_test[,covariate_names]
ehat.test <- predict(propf, X.test)$predictions
xf.preds.1.test <- predict(xf1, X.test)$predictions
xf.preds.0.test <- predict(xf0, X.test)$predictions
tauhat_xl_test <- (1 - ehat.test) * xf.preds.1.test + ehat.test * xf.preds.0.test
auhat_xl_test
tauhat_xl_test
cf <- causal_forest(
X = as.matrix(df_train[,covariate_names]),
Y = df_train$Y,
W = df_train$W,
num.trees=200) # This is just for speed. In a real application, remember increase this number!
# A good rule of thumb (for inference settings) is num.trees = number of individuals
# (nrow in our case, but would be different if using a panel dataset)
oob_pred <- predict(cf, estimate.variance=TRUE)
oob_tauhat_cf <- oob_pred$predictions
oob_tauhat_cf_se <- sqrt(oob_pred$variance.estimates)
est_pred <- predict(cf, newdata=as.matrix(df_test[covariate_names]), estimate.variance=TRUE)
tauhat_cf_test <- test_pred$predictions
tauhat_cf_test_se <- sqrt(test_pred$variance.estimates)
clear
cl
library(tidyverse)
library(tidyselect)
library(dplyr)       # Data manipulation (0.8.0.1)
library(fBasics)     # Summary statistics (3042.89)
library(corrplot)    # Correlations (0.84)
library(psych)       # Correlation p-values (1.8.12)
library(grf)         # Generalized random forests (0.10.2)
library(rpart)       # Classification and regression trees, or CART (4.1-13)
library(rpart.plot)  # Plotting trees (3.0.6)
library(treeClust)   # Predicting leaf position for causal trees (1.1-7)
library(car)         # linear hypothesis testing for causal tree (3.0-2)
library(devtools)    # Install packages from github (2.0.1)
library(readr)       # Reading csv files (1.3.1)
library(tidyr)       # Database operations (0.8.3)
library(tibble)      # Modern alternative to data frames (2.1.1)
library(knitr)       # RMarkdown (1.21)
library(kableExtra)  # Prettier RMarkdown (1.0.1)
library(ggplot2)     # general plotting tool (3.1.0)
library(haven)       # read stata files (2.0.0)
library(aod)         # hypothesis testing (1.3.1)
library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)
library(purrr)
library(causalTree)
source('load_data.R')
df_experiment <- select_dataset("welfare")
df_experiment
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
all_variables_names
df <- df_experiment %>% select(all_variables_names)
getwd()
library(tidyverse)
library(tidyselect)
library(dplyr)       # Data manipulation (0.8.0.1)
library(fBasics)     # Summary statistics (3042.89)
library(corrplot)    # Correlations (0.84)
library(psych)       # Correlation p-values (1.8.12)
library(grf)         # Generalized random forests (0.10.2)
library(rpart)       # Classification and regression trees, or CART (4.1-13)
library(rpart.plot)  # Plotting trees (3.0.6)
library(treeClust)   # Predicting leaf position for causal trees (1.1-7)
library(car)         # linear hypothesis testing for causal tree (3.0-2)
library(devtools)    # Install packages from github (2.0.1)
library(readr)       # Reading csv files (1.3.1)
library(tidyr)       # Database operations (0.8.3)
library(tibble)      # Modern alternative to data frames (2.1.1)
library(knitr)       # RMarkdown (1.21)
library(kableExtra)  # Prettier RMarkdown (1.0.1)
library(ggplot2)     # general plotting tool (3.1.0)
library(haven)       # read stata files (2.0.0)
library(aod)         # hypothesis testing (1.3.1)
library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)
library(purrr)
library(causalTree)
source('load_data.R')
df_experiment <- select_dataset("welfare")
df_experiment <- select_dataset("welfare")
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df_experiment %>% select(all_variables_names)
df <- df %>% drop_na()
df <- df %>% rename(Y=outcome_variable_name,W=treatment_variable_name)
df <- data.frame(lapply(df, function(x) as.numeric(as.character(x))))
df <- df %>% mutate_if(is.character,as.numeric)
df <- df %>% rowid_to_column( "ID")
train_fraction <- 0.80  # Use train_fraction % of the dataset to train our models
df_train <- sample_frac(df, replace=F, size=train_fraction)
df_test <- anti_join(df,df_train, by = "ID")#need to check on larger datasets
# Make a data.frame containing summary statistics of interest
summ_stats <- fBasics::basicStats(df)
summ_stats <- as.data.frame(t(summ_stats))
# Rename some of the columns for convenience
summ_stats <- summ_stats %>% select("Mean", "Stdev", "Minimum", "1. Quartile", "Median",  "3. Quartile", "Maximum")
summ_stats <- summ_stats %>% rename('Lower quartile'= '1. Quartile', 'Upper quartile' ='3. Quartile')
summ_stats
split_size <- floor(nrow(df_train) * 0.5)
df_split <- sample_n(df_train, replace=FALSE, size=split_size)
# Make the splits
df_est <- anti_join(df_train,df_split, by ="ID")
fmla_ct <- paste("factor(Y) ~", paste(covariate_names, collapse = " + "))
print('This is our regression model')
print( fmla_ct)
ct_unpruned <- honest.causalTree(
formula = fmla_ct,            # Define the model
data = df_split,              # Subset used to create tree structure
est_data = df_est,            # Which data set to use to estimate effects
treatment = df_split$W,       # Splitting sample treatment variable
est_treatment = df_est$W,     # Estimation sample treatment variable
split.Rule = "CT",            # Define the splitting option
cv.option = "TOT",            # Cross validation options
cp = 0,                       # Complexity parameter
split.Honest = TRUE,          # Use honesty when splitting
cv.Honest = TRUE,             # Use honesty when performing cross-validation
minsize = 10,                 # Min. number of treatment and control cases in each leaf
HonestSampleSize = nrow(df_est)) # Num obs used in estimation after building the tree
# Table of cross-validated values by tuning parameter.
ct_cptable <- as.data.frame(ct_unpruned$cptable)
# Obtain optimal complexity parameter to prune tree.
selected_cp <- which.min(ct_cptable$xerror)
optim_cp_ct <- ct_cptable[selected_cp, "CP"]
# Prune the tree at optimal complexity parameter.
ct_pruned <- prune(tree = ct_unpruned, cp = optim_cp_ct)
tauhat_ct_est <- predict(ct_pruned, newdata = df_est)
# Create a factor column 'leaf' indicating leaf assignment
num_leaves <- length(unique(tauhat_ct_est))  #There are as many leaves as there are predictions
df_est$leaf <- factor(tauhat_ct_est, labels = seq(num_leaves))
# Run the regression
ols_ct <- lm(as.formula("Y ~ 0 + leaf + W:leaf"), data= df_est) #Warning: the tree won't split for charitable dataset
print(as.formula("Y ~ 0 + leaf + W:leaf"))
#askNK:charitable does not split
ols_ct_summary <- summary(ols_ct)
te_summary <- coef(ols_ct_summary)[(num_leaves+1):(2*num_leaves), c("Estimate", "Std. Error")]
tauhat_ct_test <- predict(ct_pruned, newdata=df_test)
rpart.plot(
x = ct_pruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
clear
library(tidyverse)
library(tidyselect)
library(dplyr)       # Data manipulation (0.8.0.1)
library(fBasics)     # Summary statistics (3042.89)
library(corrplot)    # Correlations (0.84)
library(psych)       # Correlation p-values (1.8.12)
library(grf)         # Generalized random forests (0.10.2)
library(rpart)       # Classification and regression trees, or CART (4.1-13)
library(rpart.plot)  # Plotting trees (3.0.6)
library(treeClust)   # Predicting leaf position for causal trees (1.1-7)
library(car)         # linear hypothesis testing for causal tree (3.0-2)
library(devtools)    # Install packages from github (2.0.1)
library(readr)       # Reading csv files (1.3.1)
library(tidyr)       # Database operations (0.8.3)
library(tibble)      # Modern alternative to data frames (2.1.1)
library(knitr)       # RMarkdown (1.21)
library(kableExtra)  # Prettier RMarkdown (1.0.1)
library(ggplot2)     # general plotting tool (3.1.0)
library(haven)       # read stata files (2.0.0)
library(aod)         # hypothesis testing (1.3.1)
library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)
library(purrr)
library(causalTree)
source('load_data.R')
df_experiment <- select_dataset("welfare")
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df_experiment %>% select(all_variables_names)
df <- df %>% drop_na()
df <- df %>% rename(Y=outcome_variable_name,W=treatment_variable_name)
df <- data.frame(lapply(df, function(x) as.numeric(as.character(x))))
df <- df %>% mutate_if(is.character,as.numeric)
df <- df %>% rowid_to_column( "ID")
train_fraction <- 0.80  # Use train_fraction % of the dataset to train our models
df_train <- sample_frac(df, replace=F, size=train_fraction)
df_test <- anti_join(df,df_train, by = "ID")#need to check on larger datasets
summ_stats <- fBasics::basicStats(df)
summ_stats <- as.data.frame(t(summ_stats))
# Rename some of the columns for convenience
summ_stats <- summ_stats %>% select("Mean", "Stdev", "Minimum", "1. Quartile", "Median",  "3. Quartile", "Maximum")
summ_stats <- summ_stats %>% rename('Lower quartile'= '1. Quartile', 'Upper quartile' ='3. Quartile')
split_size <- floor(nrow(df_train) * 0.5)
df_split <- sample_n(df_train, replace=FALSE, size=split_size)
# Make the splits
df_est <- anti_join(df_train,df_split, by ="ID")
fmla_ct <- paste("factor(Y) ~", paste(covariate_names, collapse = " + "))
print('This is our regression model')
print( fmla_ct)
ct_unpruned <- honest.causalTree(
formula = fmla_ct,            # Define the model
data = df_split,              # Subset used to create tree structure
est_data = df_est,            # Which data set to use to estimate effects
treatment = df_split$W,       # Splitting sample treatment variable
est_treatment = df_est$W,     # Estimation sample treatment variable
split.Rule = "CT",            # Define the splitting option
cv.option = "TOT",            # Cross validation options
cp = 0,                       # Complexity parameter
split.Honest = TRUE,          # Use honesty when splitting
cv.Honest = TRUE,             # Use honesty when performing cross-validation
minsize = 10,                 # Min. number of treatment and control cases in each leaf
HonestSampleSize = nrow(df_est)) # Num obs used in estimation after building the tree
rpart.plot(
x = ct_unpruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
# Table of cross-validated values by tuning parameter.
ct_cptable <- as.data.frame(ct_unpruned$cptable)
# Obtain optimal complexity parameter to prune tree.
selected_cp <- which.min(ct_cptable$xerror)
optim_cp_ct <- ct_cptable[selected_cp, "CP"]
# Prune the tree at optimal complexity parameter.
ct_pruned <- prune(tree = ct_unpruned, cp = optim_cp_ct)
rpart.plot(
x = ct_pruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
# Table of cross-validated values by tuning parameter.
ct_cptable <- as.data.frame(ct_unpruned$cptable)
# Obtain optimal complexity parameter to prune tree.
selected_cp <- which.min(ct_cptable$xerror)
optim_cp_ct <- ct_cptable[selected_cp, "CP"]
# Prune the tree at optimal complexity parameter.
ct_pruned <- prune(tree = ct_unpruned, cp = optim_cp_ct)
rpart.plot(
x = ct_pruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
library(tidyverse)
library(tidyselect)
library(dplyr)       # Data manipulation (0.8.0.1)
library(fBasics)     # Summary statistics (3042.89)
library(corrplot)    # Correlations (0.84)
library(psych)       # Correlation p-values (1.8.12)
library(grf)         # Generalized random forests (0.10.2)
library(rpart)       # Classification and regression trees, or CART (4.1-13)
library(rpart.plot)  # Plotting trees (3.0.6)
library(treeClust)   # Predicting leaf position for causal trees (1.1-7)
library(car)         # linear hypothesis testing for causal tree (3.0-2)
library(devtools)    # Install packages from github (2.0.1)
library(readr)       # Reading csv files (1.3.1)
library(tidyr)       # Database operations (0.8.3)
library(tibble)      # Modern alternative to data frames (2.1.1)
library(knitr)       # RMarkdown (1.21)
library(kableExtra)  # Prettier RMarkdown (1.0.1)
library(ggplot2)     # general plotting tool (3.1.0)
library(haven)       # read stata files (2.0.0)
library(aod)         # hypothesis testing (1.3.1)
library(evtree)      # evolutionary learning of globally optimal trees (1.0-7)
library(purrr)
library(causalTree)
source('load_data.R')
df_experiment <- select_dataset("welfare")
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df_experiment %>% select(all_variables_names)
df <- df %>% drop_na()
df <- df %>% rename(Y=outcome_variable_name,W=treatment_variable_name)
df <- data.frame(lapply(df, function(x) as.numeric(as.character(x))))
df <- df %>% mutate_if(is.character,as.numeric)
df <- df %>% rowid_to_column( "ID")
train_fraction <- 0.80  # Use train_fraction % of the dataset to train our models
df_train <- sample_frac(df, replace=F, size=train_fraction)
df_test <- anti_join(df,df_train, by = "ID")#need to check on larger datasets
summ_stats <- fBasics::basicStats(df)
summ_stats <- as.data.frame(t(summ_stats))
# Rename some of the columns for convenience
summ_stats <- summ_stats %>% select("Mean", "Stdev", "Minimum", "1. Quartile", "Median",  "3. Quartile", "Maximum")
summ_stats <- summ_stats %>% rename('Lower quartile'= '1. Quartile', 'Upper quartile' ='3. Quartile')
# Diving the data 40%-40%-20% into splitting, estimation and validation samples
split_size <- floor(nrow(df_train) * 0.5)
df_split <- sample_n(df_train, replace=FALSE, size=split_size)
# Make the splits
df_est <- anti_join(df_train,df_split, by ="ID")
fmla_ct <- paste("factor(Y) ~", paste(covariate_names, collapse = " + "))
print('This is our regression model')
print( fmla_ct)
ct_unpruned <- honest.causalTree(
formula = fmla_ct,            # Define the model
data = df_split,              # Subset used to create tree structure
est_data = df_est,            # Which data set to use to estimate effects
treatment = df_split$W,       # Splitting sample treatment variable
est_treatment = df_est$W,     # Estimation sample treatment variable
split.Rule = "CT",            # Define the splitting option
cv.option = "TOT",            # Cross validation options
cp = 0,                       # Complexity parameter
split.Honest = TRUE,          # Use honesty when splitting
cv.Honest = TRUE,             # Use honesty when performing cross-validation
minsize = 10,                 # Min. number of treatment and control cases in each leaf
HonestSampleSize = nrow(df_est)) # Num obs used in estimation after building the tree
ct_cptable <- as.data.frame(ct_unpruned$cptable)
# Obtain optimal complexity parameter to prune tree.
selected_cp <- which.min(ct_cptable$xerror)
optim_cp_ct <- ct_cptable[selected_cp, "CP"]
# Prune the tree at optimal complexity parameter.
ct_pruned <- prune(tree = ct_unpruned, cp = optim_cp_ct)
rpart.plot(
x = ct_pruned,        # Pruned tree
type = 3,             # Draw separate split labels for the left and right directions
fallen = TRUE,        # Position the leaf nodes at the bottom of the graph
leaf.round = 1,       # Rounding of the corners of the leaf node boxes
extra = 100,          # Display the percentage of observations in the node
branch = 0.1,          # Shape of the branch lines
box.palette = "RdBu") # Palette for coloring the node
